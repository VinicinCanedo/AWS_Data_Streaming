# üìä Serverless Datalake com AWS ‚Äî Streaming de Dados da Bolsa de Valores

Este projeto demonstra como construir uma arquitetura **serverless** para ingest√£o e processamento de dados da bolsa de valores dos EUA em tempo real, utilizando servi√ßos gerenciados da AWS. A proposta √© criar uma pipeline escal√°vel, eficiente e de baixo custo, com foco total na l√≥gica de neg√≥cios.

---
üìä Fonte de Dados (Data Source)
O projeto utiliza uma API de mercado financeiro para o consumo de dados via WebSockets. A arquitetura foi desenhada para lidar com as seguintes caracter√≠sticas da fonte:

Streaming em Tempo Real (Push Data): Diferente de APIs baseadas em polling (onde o sistema precisa pedir a informa√ß√£o), utilizamos o modelo Push, onde o servidor envia os dados instantaneamente assim que uma transa√ß√£o ocorre, garantindo baix√≠ssima lat√™ncia.

Comportamento Multi-Ativo: O sistema processa ativos de diferentes naturezas:

A√ß√µes (Stocks): Dados centralizados em bolsas (ex: NYSE).

Forex e Criptomoedas: Como s√£o mercados descentralizados, o pipeline trata atualiza√ß√µes informativas de pre√ßo. Quando um registro possui volume zero, o sistema o interpreta como uma atualiza√ß√£o de cota√ß√£o (tick) e n√£o necessariamente uma transa√ß√£o executada.

Efici√™ncia de Banda (Batching): A API agrupa m√∫ltiplas negocia√ß√µes em um √∫nico payload JSON para otimizar o tr√°fego. O c√≥digo de ingest√£o est√° preparado para processar essas listas de registros de forma eficiente antes do envio ao AWS Kinesis.

üí° Contexto de Neg√≥cio e Arquitetura
A ideia central do projeto √© criar uma infraestrutura robusta de Data Ingestion capaz de escalar conforme o volume do mercado financeiro mundial.

Ingest√£o de Alta Frequ√™ncia: Capturar e padronizar fluxos de dados heterog√™neos para an√°lise posterior ou dashboards em tempo real.

Resili√™ncia e Seguran√ßa: O sistema implementa um controle de sess√£o rigoroso para respeitar a pol√≠tica de conex√£o √∫nica por chave de API, evitando quedas de servi√ßo e garantindo a integridade do fluxo de dados.

Processamento Escal√°vel: Ao utilizar o AWS Kinesis como porta de entrada, o projeto demonstra a capacidade de desacoplar a fonte de dados (API) dos consumidores (bancos de dados, lambdas ou analytics), permitindo crescimento horizontal.
---
## üöÄ Vis√£o Geral

A arquitetura foi desenhada para consumir dados de trade em tempo real e armazen√°-los de forma otimizada, sem a necessidade de gerenciar servidores ou infraestrutura complexa. Utilizando o modelo **Serverless Datalake**, o projeto explora o poder da nuvem para entregar:

- Alta escalabilidade  
- Baixo custo operacional  
- Alta disponibilidade  
- Foco total na l√≥gica de dados  

---

## üß∞ Tecnologias Utilizadas

| Servi√ßo / Ferramenta | Finalidade |
|----------------------|------------|
| **Java**             | Producer local para envio dos dados |
| **AWS Kinesis**      | Ingest√£o e processamento em tempo real |
| **AWS Lambda**       | Processamento e persist√™ncia dos dados |
| **AWS DynamoDB**     | Armazenamento NoSQL escal√°vel |
| **IAM**              | Controle de acesso entre servi√ßos |
| **AWS CLI**          | Provisionamento e automa√ß√£o |

---

## üõ†Ô∏è Passo a Passo para Reproduzir o Projeto

### 1Ô∏è‚É£ Criar uma conta no Finnhub e obter a API Key

- Acesse [https://finnhub.io](https://finnhub.io) e crie uma conta gratuita.
- Ap√≥s o login, gere sua **API_KEY**.
- No projeto Java, insira essa chave no arquivo `application.properties`:

```properties
finnhub.api.key=YOUR_API_KEY
```

- Compile o projeto e gere o `.jar` com sua aplica√ß√£o de streaming.

---

### 2Ô∏è‚É£ Instalar e configurar o AWS CLI

- Baixe o AWS CLI: [https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
- Configure com suas credenciais:

```bash
aws configure
```

---

### 3Ô∏è‚É£ Criar um Kinesis Data Stream

- No console da AWS ou via CLI, crie um stream chamado:

```bash
stream-finnhub
```

- Esse ser√° o canal de ingest√£o dos dados de trade.

---

### 4Ô∏è‚É£ Criar uma fun√ß√£o IAM para o Lambda

- V√° at√© o IAM e crie uma fun√ß√£o chamada:

```text
FinnhubStreamProcessorRole
```

- Anexe a pol√≠tica gerenciada:

```text
AWSLambdaKinesisExecutionRole
```

- Crie uma pol√≠tica personalizada chamada `PolicyWriteDynamoDB` com as permiss√µes:

```json
{
  "Effect": "Allow",
  "Action": [
    "dynamodb:PutItem",
    "dynamodb:BatchWriteItem"
  ],
  "Resource": "arn:aws:dynamodb:<REGIAO>:<ID_CONTA>:table/ProcessedTradesDynamoDB"
}
```

- Anexe essa pol√≠tica √† fun√ß√£o.

---

### 5Ô∏è‚É£ Criar a tabela no DynamoDB

- Nome sugerido: `ProcessedTradesDynamoDB`
- Chave de parti√ß√£o obrigat√≥ria: `trade_id` (tipo: String)

> Voc√™ pode usar outro nome para a tabela, mas o campo `trade_id` deve existir como chave prim√°ria.

---

### 6Ô∏è‚É£ Criar a fun√ß√£o Lambda

- Crie uma fun√ß√£o Lambda com o runtime Python 3.13.
- Configure o gatilho como o stream `stream-finnhub`.
- Use o c√≥digo `lambda_function.py` presente neste reposit√≥rio.
- Adicione uma vari√°vel de ambiente chamada:

```text
DYNAMODB_TABLE_NAME = ProcessedTradesDynamoDB
```

- Vincule a fun√ß√£o √† role `FinnhubStreamProcessorRole`.

---

### üßæ Observa√ß√µes

- O arquivo `consumer.py` est√° presente apenas para fins ilustrativos e **n√£o precisa ser executado**.
- O projeto foi pensado para operar durante o hor√°rio de mercado, otimizando custos com arquitetura serverless.

---

## üìö Fontes e Inspira√ß√£o

- [Pixegami - YouTube](https://www.youtube.com/watch?v=CjVPMocEECM)  
- [AWS Lambda Stream Processing](https://github.com/aws-samples/lambda-refarch-streamprocessing)  
- [Kinesis Data Analytics Blueprints](https://github.com/aws-samples/amazon-kinesis-data-analytics-blueprints/tree/main/apps/java-datastream/kds-to-s3-datastream-java)

---
